{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 테슬라 주가 데이터 다운로드 (예: 최근 5년간의 일일 주가)\n",
    "ticker = \"TSLA\"\n",
    "data = yf.download(ticker, period=\"5y\", interval=\"1d\")\n",
    "data = data[['Close']]  # 종가만 사용\n",
    "print(data.head())\n",
    "\n",
    "# 데이터 정규화 (0과 1 사이로 변환)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# 시퀀스 데이터 준비 함수\n",
    "def create_sequences(dataset, time_step=60):\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset) - time_step - 1):\n",
    "        X.append(dataset[i:(i + time_step), 0])\n",
    "        y.append(dataset[i + time_step, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_step = 60\n",
    "X, y = create_sequences(scaled_data, time_step)\n",
    "\n",
    "# PyTorch 텐서로 변환 및 차원 재조정\n",
    "X = torch.from_numpy(X).float().unsqueeze(-1)  # shape: (samples, timesteps, features)\n",
    "y = torch.from_numpy(y).float().unsqueeze(-1)  # shape: (samples, 1)\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM 레이어 정의\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # 출력층 정의\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 초기 은닉 상태 및 셀 상태 정의 (batch_size, hidden_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # LSTM 순전파: out shape -> (batch, seq_length, hidden_size)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # 마지막 타임스텝의 출력을 사용하여 예측\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "input_size = 1\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "\n",
    "# 모델 초기화\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# 장치 설정 (GPU 사용 가능 시 GPU 사용)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    # 에포크별 학습 손실 출력\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {np.mean(train_losses):.6f}\")\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        predictions.append(outputs.cpu().numpy())\n",
    "        actuals.append(batch_y.numpy())\n",
    "\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "actuals = np.concatenate(actuals, axis=0)\n",
    "\n",
    "# 정규화 해제 (원래 값 복원)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "actuals = scaler.inverse_transform(actuals)\n",
    "\n",
    "# 실제 날짜 인덱스 가져오기 (테스트 데이터 부분)\n",
    "test_data = data.iloc[-len(actuals):]  # 실제 test 기간에 해당하는 데이터\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(test_data.index, actuals, label='Test Actual')\n",
    "plt.plot(test_data.index, predictions, label='Test Prediction')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Tesla Close Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
